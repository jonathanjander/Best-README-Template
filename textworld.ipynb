{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathanjander/Best-README-Template/blob/master/textworld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KygCLbPDOPQ"
      },
      "source": [
        "# Textworld example\n",
        "\n",
        "## notes\n",
        "- [official pytorch example](https://colab.research.google.com/github/microsoft/TextWorld/blob/main/notebooks/Building%20a%20simple%20agent.ipynb#scrollTo=ChrM9GGGlrtf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOK2hjFEC8tx",
        "outputId": "0deb08fe-86cd-4489-e8c3-cc3647489e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for jericho (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jericho: filename=jericho-3.1.2-py3-none-any.whl size=325097 sha256=44944d71ba796eebe4835670c53ed21560a982b5dac6e7886b5f1f1210a4c55b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1d/a7/91e11767b583fe77fae27d292e724d0dc8cd4335dab886adfe\n",
            "Successfully built textworld jericho\n",
            "Installing collected packages: mementos, tatsu, hashids, jericho, textworld\n",
            "Successfully installed hashids-1.3.1 jericho-3.1.2 mementos-1.3.1 tatsu-5.8.3 textworld-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install textworld\n",
        "#!pip install gym\n",
        "#!pip install gym==0.21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D6GjCzJMEM-i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from glob import glob\n",
        "import os\n",
        "import re\n",
        "from typing import List, Mapping, Any, Optional\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import textworld\n",
        "import textworld.gym\n",
        "from textworld import EnvInfos\n",
        "from time import time\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j43nOrbzGOLx"
      },
      "source": [
        "## init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BajFp_UAGXr5",
        "outputId": "cbec3d5d-8b6b-4c6b-ee59-1254b2ce5721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed: 42\n",
            "Game generated: /content/games/tw-rewardsDense_goalDetailed.z8\n",
            "\n",
            "Objective:\n",
            "I hope you're ready to go into rooms and interact with objects, because you've just entered TextWorld! Here is how to play! First stop, open the chest drawer. After that, recover the old key from the chest drawer within the bedroom. Then, check that the wooden door is unlocked with the old key. And then, open the wooden door inside the bedroom. Then, go to the east. And then, look and see that the screen door in the kitchen is ajar. After that, make an effort to venture east. And then, attempt to move south. With that accomplished, lift the bell pepper from the floor of the garden. After that, move north. Okay, and then, attempt to take a trip west. Following that, rest the bell pepper on the stove. And once you've done that, you win!\n",
            "\n",
            "Walkthrough:\n",
            "open chest drawer > take old key from chest drawer > unlock wooden door with old key > open wooden door > go east > open screen door > go east > go south > take bell pepper > go north > go west > put bell pepper on stove\n",
            "\n",
            "-= Stats =-\n",
            "Nb. locations: 6\n",
            "Nb. objects: 28\n"
          ]
        }
      ],
      "source": [
        "#!tw-make custom --world-size 2 --quest-length 4 --nb-objects 10 --output tw_games/game.ulx -f -v --seed 123\n",
        "!tw-make tw-simple --rewards dense  --goal detailed --seed 42 --output games/tw-rewardsDense_goalDetailed.z8 -v -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mYslX3VGMVb"
      },
      "outputs": [],
      "source": [
        "request_infos = textworld.EnvInfos(\n",
        "    admissible_commands=True,  # All commands relevant to the current state.\n",
        "    entities=True,             # List of all interactable entities found in the game.\n",
        "    facts=True,  # All the facts that are currently true about the world.\n",
        "    intermediate_reward=True,\n",
        "    max_score = True,\n",
        "    inventory=True,\n",
        "    description=True,\n",
        "    command_templates = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnu5jFOQGMYG",
        "outputId": "9130a13d-d235-43af-951a-351e65554d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entities: ['wooden door', 'screen door', 'chest drawer', 'antique trunk', 'refrigerator', 'toilet', 'bath', 'lettuce', 'bell pepper', 'apple', 'shovel', 'king-size bed', 'counter', 'set of chairs', 'stove', 'kitchen island', 'sink', 'couch', 'low table', 'tv', 'bbq', 'patio table', 'tomato plant', 'half of a bag of chips', 'milk', 'old key', 'soap bar', 'toothbrush', 'remote', 'note', 'north', 'south', 'east', 'west']\n",
            "\n",
            "Admissible commands:\n",
            "  examine antique trunk\n",
            "  examine chest drawer\n",
            "  examine king-size bed\n",
            "  examine wooden door\n",
            "  inventory\n",
            "  look\n",
            "  open antique trunk\n",
            "  open chest drawer\n"
          ]
        }
      ],
      "source": [
        "# Requesting additional information should be done when registering the game.\n",
        "#env_id = textworld.gym.register_game('tw_games/game.ulx', request_infos)\n",
        "env_id = textworld.gym.register_game(\"./games/tw-rewardsDense_goalDetailed.z8\", request_infos)\n",
        "env = textworld.gym.make(env_id)\n",
        "\n",
        "obs, infos = env.reset()\n",
        "print(\"Entities: {}\\n\".format(infos[\"entities\"]))\n",
        "print(\"Admissible commands:\\n  {}\".format(\"\\n  \".join(infos[\"admissible_commands\"])))\n",
        "#print(\"command_templates:\\n  {}\".format(\"\\n  \".join(infos[\"command_templates\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11ARgwqTvguA"
      },
      "outputs": [],
      "source": [
        "action_space = len(infos[\"admissible_commands\"])\n",
        "state_space = 20 # number of rooms times number of items 2*10 (dont think this is correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1onOUgXTEFM"
      },
      "source": [
        "### playing the game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxs7s-XJNVTS"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    done = False\n",
        "    obs, _ = env.reset()\n",
        "    print(obs)\n",
        "\n",
        "    print(infos[\"admissible_commands\"])\n",
        "    nb_moves = 0\n",
        "    while not done:\n",
        "        command = input(\"> \")\n",
        "        obs, score, done, infos = env.step(command)\n",
        "        print(obs)\n",
        "        print('Score',score)\n",
        "        print(infos[\"admissible_commands\"])\n",
        "        nb_moves += 1\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass  # Press the stop button in the toolbar to quit the game.\n",
        "\n",
        "print(\"Played {} steps, scoring {} points.\".format(nb_moves, score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oirKK-onj_5G"
      },
      "source": [
        "# trying to implement the example code to tensorflow (my implementation)\n",
        "- using a GRU\n",
        "- using embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dKRyUNxSRIa"
      },
      "source": [
        "### main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-bTN11fkJhb"
      },
      "outputs": [],
      "source": [
        "# building a random baseline\n",
        "class RandomAgent(textworld.gym.Agent):\n",
        "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
        "    def __init__(self, seed=1234):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.RandomState(self.seed)\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> textworld.EnvInfos:\n",
        "        return textworld.EnvInfos(admissible_commands=True)\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
        "        return self.rng.choice(infos[\"admissible_commands\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "92ElBvEpkJkm"
      },
      "outputs": [],
      "source": [
        "# play function\n",
        "def play(agent, path, max_step=100, nb_episodes=10, verbose=True):\n",
        "    # torch.manual_seed(20211021)  # For reproducibility when using action sampling.\n",
        "\n",
        "    infos_to_request = agent.infos_to_request\n",
        "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
        "\n",
        "    gamefiles = [path]\n",
        "    if os.path.isdir(path):\n",
        "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
        "\n",
        "    env_id = textworld.gym.register_games(gamefiles,\n",
        "                                          request_infos=infos_to_request,\n",
        "                                          max_episode_steps=max_step)\n",
        "    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            print(os.path.dirname(path), end=\"\")\n",
        "        else:\n",
        "            print(os.path.basename(path), end=\"\")\n",
        "\n",
        "    # Collect some statistics: nb_steps, final reward.\n",
        "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
        "    for no_episode in range(nb_episodes):\n",
        "        obs, infos = env.reset()  # Start new episode.\n",
        "\n",
        "        score = 0\n",
        "        done = False\n",
        "        nb_moves = 0\n",
        "        while not done:\n",
        "            command = agent.act(obs, score, done, infos, no_episode) # CHANGE FOR TF\n",
        "            #command = agent.act(obs, done, infos)\n",
        "            obs, score, done, infos = env.step(command)\n",
        "            nb_moves += 1\n",
        "\n",
        "        agent.train_model()\n",
        "        agent.act(obs, score, done, infos, no_episode)  # Let the agent know the game is done. CHANGE FOR TF\n",
        "        # agent.act(obs, done, infos)  # Let the agent know the game is done. CHANGE FOR TF\n",
        "\n",
        "\n",
        "        if verbose:\n",
        "            print(\".\", end=\"\")\n",
        "        avg_moves.append(nb_moves)\n",
        "        avg_scores.append(score)\n",
        "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
        "\n",
        "    env.close()\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
        "        else:\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT1Q0fHTmcOP"
      },
      "outputs": [],
      "source": [
        "#agent = RandomAgent()\n",
        "#play(agent, 'tw_games/game.ulx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VihokiidmNp8"
      },
      "outputs": [],
      "source": [
        "# TENSORFLOW\n",
        "# TODO INCREASE PERFORMANCE\n",
        "class CommandScorer(models.Model):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CommandScorer, self).__init__()\n",
        "        self.embedding = layers.Embedding(input_size, hidden_size)\n",
        "        self.encoder_gru = layers.GRU(hidden_size, return_sequences=True, return_state=True)\n",
        "        self.cmd_encoder_gru = layers.GRU(hidden_size, return_state=True)\n",
        "        self.state_gru = layers.GRU(hidden_size, return_state=True)\n",
        "        self.critic = layers.Dense(1) # Critic for state value estimation\n",
        "        self.att_cmd = layers.Dense(1) # Attention mechanism for commands\n",
        "\n",
        "        self.hidden_size  = hidden_size\n",
        "\n",
        "        # defining states\n",
        "        # Initialize hidden states for GRU layers\n",
        "        self.encoder_state = tf.Variable(tf.zeros([1, hidden_size]), trainable=False)\n",
        "        self.cmd_encoder_state = tf.Variable(tf.zeros([1, hidden_size]), trainable=False)\n",
        "        self.state_gru_state = tf.Variable(tf.zeros([1, hidden_size]), trainable=False)\n",
        "\n",
        "    def call(self, obs, commands):\n",
        "        # Process observation\n",
        "        embedded_obs = self.embedding(obs)\n",
        "\n",
        "        #_, encoder_hidden = self.encoder_gru(embedded_obs) # stateless\n",
        "        encoder_output, encoder_hidden = self.encoder_gru(embedded_obs, initial_state=self.encoder_state) # stateful\n",
        "        self.encoder_state.assign(encoder_hidden) # stateful\n",
        "\n",
        "        # Expand dimensions of encoder_hidden to fit GRU input requirements\n",
        "        # for critic state value prediction\n",
        "        encoder_hidden_expanded = tf.expand_dims(encoder_hidden, axis=1)\n",
        "\n",
        "        #state_output, _ = self.state_gru(encoder_hidden_expanded) # stateless\n",
        "        # Process state GRU with state handling\n",
        "        state_output, new_state_gru_state = self.state_gru(encoder_hidden_expanded, initial_state=self.state_gru_state) # stateful\n",
        "        self.state_gru_state.assign(new_state_gru_state) # stateful\n",
        "\n",
        "        #critic prediction\n",
        "        value = self.critic(state_output)\n",
        "\n",
        "        # Process commands\n",
        "        cmds_embedding = self.embedding(commands)  # Shape: (num_commands, cmd_length, hidden_size)\n",
        "        cmd_length = cmds_embedding.shape[1]\n",
        "\n",
        "        # Reshape for batch processing\n",
        "        cmds_embedding_reshaped = tf.reshape(cmds_embedding, (-1, cmd_length, self.embedding.output_dim))\n",
        "        batch_size = tf.shape(cmds_embedding_reshaped)[0]\n",
        "\n",
        "        # Process each command as a separate sequence (batch)\n",
        "        _, cmds_encoding_last_states = self.cmd_encoder_gru(cmds_embedding_reshaped, initial_state=tf.zeros((batch_size, self.encoder_gru.units))) # stateless\n",
        "\n",
        "\n",
        "        # _, cmds_encoding_last_states = self.cmd_encoder_gru(cmds_embedding_reshaped, initial_state=self.cmd_encoder_state) # stateful\n",
        "        #self.state_gru_state.assign(cmds_encoding_last_states) # stateful\n",
        "\n",
        "        # Reshape to get separate encodings for each command\n",
        "        cmds_encoding_last_states = tf.reshape(cmds_encoding_last_states, (1, -1, self.encoder_gru.units))\n",
        "\n",
        "        # Prepare state representation\n",
        "        state_hidden_repeated = tf.repeat(encoder_hidden_expanded, repeats=tf.shape(commands)[0], axis=1)\n",
        "\n",
        "        # Concatenate state and command encodings\n",
        "        cmd_selector_input = tf.concat([state_hidden_repeated, cmds_encoding_last_states], axis=-1)\n",
        "\n",
        "        #print('state_hidden_repeated', state_hidden_repeated)\n",
        "        # Compute scores and select action\n",
        "        scores = self.att_cmd(cmd_selector_input)\n",
        "        scores = tf.squeeze(scores, axis=-1)\n",
        "\n",
        "        #print('scores', scores)\n",
        "\n",
        "        # Calculate probabilities and sample an action\n",
        "        probs = tf.nn.softmax(scores, axis=1)\n",
        "        index = tf.random.categorical(tf.math.log(probs), num_samples=1)\n",
        "\n",
        "        #print(scores)\n",
        "\n",
        "        return scores, index, value\n",
        "\n",
        "    def reset_hidden(self, batch_size):\n",
        "        # Reset hidden states\n",
        "        self.encoder_state.assign(tf.zeros([batch_size, self.hidden_size]))\n",
        "        self.cmd_encoder_state.assign(tf.zeros([batch_size, self.hidden_size]))\n",
        "        self.state_gru_state.assign(tf.zeros([batch_size, self.hidden_size]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VcJNXqGw2r1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "840Ao-bVlGe1"
      },
      "outputs": [],
      "source": [
        "# TENSORFLOW\n",
        "\n",
        "class NeuralAgent:\n",
        "    # ... (Initialization and utility functions remain largely the same)\n",
        "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
        "    MAX_VOCAB_SIZE = 1000\n",
        "    UPDATE_FREQUENCY = 10\n",
        "    LOG_FREQUENCY = 1000\n",
        "    GAMMA = 0.9\n",
        "    LR = 0.00003\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, oov_token=\"<UNK>\")\n",
        "        self._initialized = False\n",
        "        self._epsiode_has_started = False\n",
        "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
        "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
        "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), 0.00003) # CHANGE FOR TF\n",
        "        self.optimizer = tf.optimizers.Adam(learning_rate=self.LR)\n",
        "        self.mode = \"test\"\n",
        "\n",
        "\n",
        "    def train(self): # CHANGE FOR TF\n",
        "        self.mode = \"train\"\n",
        "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "        self.transitions = []\n",
        "        self.model.reset_hidden(1)\n",
        "        self.last_score = 0\n",
        "        self.no_train_step = 0\n",
        "\n",
        "    def test(self): # CHANGE FOR TF\n",
        "        self.mode = \"test\"\n",
        "        #self.model.reset_hidden(1)\n",
        "\n",
        "    def train_model(self):\n",
        "      return\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> EnvInfos: # WORKING\n",
        "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
        "                        won=True, lost=True)\n",
        "\n",
        "    def _get_word_id(self, word): # WORKING\n",
        "\n",
        "        #print('GET WORD ID METHOD')\n",
        "        if word not in self.word2id:\n",
        "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
        "                return self.word2id[\"<UNK>\"]\n",
        "\n",
        "            self.id2word.append(word)\n",
        "            self.word2id[word] = len(self.word2id)\n",
        "\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def _tokenize(self, text): # WORKING\n",
        "\n",
        "        #print('TOKENIZE METHOD')\n",
        "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
        "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
        "        word_ids = list(map(self._get_word_id, text.split()))\n",
        "        return word_ids\n",
        "\n",
        "    def _process(self, texts, tokenizer): # WORKING\n",
        "\n",
        "        #print('PROCESS METHOD')\n",
        "        #tokenized_texts = tokenizer.texts_to_sequences(texts)\n",
        "        texts = list(map(self._tokenize, texts))\n",
        "        max_len = max(len(l) for l in texts)\n",
        "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            padded[i, :len(text)] = text\n",
        "        #padded_texts = tf.keras.preprocessing.sequence.pad_sequences(tokenized_texts, maxlen=max_len)\n",
        "\n",
        "        # Convert the NumPy array to a TensorFlow tensor\n",
        "        padded_tensor = tf.convert_to_tensor(padded, dtype=tf.int32)\n",
        "\n",
        "        # Transpose the tensor to switch from Batch x Sequence to Sequence x Batch\n",
        "        #padded_tensor = tf.transpose(padded_tensor, perm=[1, 0])\n",
        "\n",
        "        return padded\n",
        "\n",
        "    def _compute_advantage(self, last_values): # TF\n",
        "      returns, advantages = [], []\n",
        "      R = last_values\n",
        "      for t in reversed(range(len(self.transitions))):\n",
        "          rewards, _, _, values = self.transitions[t]\n",
        "          R = rewards + self.GAMMA * R\n",
        "          adv = R - values\n",
        "          returns.append(R)\n",
        "          advantages.append(adv)\n",
        "\n",
        "      return returns[::-1], advantages[::-1]\n",
        "\n",
        "\n",
        "    def _debug_train(self, input_tens, command_tens, infos, tape):\n",
        "      debug_target = tf.random.normal(shape=[1, len(infos[\"admissible_commands\"])])\n",
        "      #with tf.GradientTape() as tape:\n",
        "\n",
        "      # Forward pass: Get model's output for the dummy input\n",
        "      debug_output, _, _ = self.model(input_tens, command_tens)\n",
        "      #print('output',debug_output)\n",
        "\n",
        "      # Simplified loss: MSE between model's output and the arbitrary target\n",
        "      debug_loss = tf.reduce_mean(tf.square(debug_output - debug_target))\n",
        "\n",
        "\n",
        "      # Compute gradients\n",
        "      debug_gradients = tape.gradient(debug_loss, self.model.trainable_variables)\n",
        "\n",
        "      # Check gradients\n",
        "      print(\"Debug Loss:\", debug_loss.numpy())\n",
        "      #print(\"Debug Gradients:\", len(debug_gradients))\n",
        "\n",
        "      # Apply gradients if they are valid (not None)\n",
        "\n",
        "      # Print debugging information\n",
        "      #print(\"Debug Loss:\", debug_loss.numpy())\n",
        "\n",
        "\n",
        "      # Apply only non-None gradients\n",
        "      gradients_to_apply = [(grad, var) for grad, var in zip(debug_gradients, self.model.trainable_variables) if grad is not None]\n",
        "\n",
        "      if gradients_to_apply:\n",
        "        self.optimizer.apply_gradients(gradients_to_apply)\n",
        "        #print(\"Some valid gradients applied.\")\n",
        "      else:\n",
        "        print(\"No valid gradients. Check model computations.\")\n",
        "\n",
        "\n",
        "    def _train_loop(self, values, tape):\n",
        "\n",
        "      returns, advantages = self._compute_advantage(values)\n",
        "\n",
        "      #with tf.GradientTape() as tape:\n",
        "      loss = 0\n",
        "      # actor critic policy gradient using advantage\n",
        "      # using advantage rather than the raw reward reduces variance\n",
        "      for transition, ret, adv in zip(self.transitions, returns, advantages):\n",
        "        #print('trans: ',self.transitions)\n",
        "        reward, indexes_, outputs_, values_ = transition\n",
        "        # indexes_ is the index of the action\n",
        "        # output_ is the logits\n",
        "        # values_ is the predicted value\n",
        "\n",
        "        #print('reward', type(reward))\n",
        "        #print('indexes_', type(indexes_))\n",
        "        #print('outputs_', type(outputs_))\n",
        "        #print('values_', type(values_))\n",
        "\n",
        "        # Calculate policy loss\n",
        "        probs = tf.nn.softmax(outputs_)\n",
        "        log_probs = tf.math.log(probs)\n",
        "        log_action_probs = tf.reduce_sum(tf.one_hot(indexes_, outputs_.shape[-1]) * log_probs)\n",
        "\n",
        "        # Calculate policy loss\n",
        "        # this is the ACTOR part\n",
        "        policy_loss = -log_action_probs * adv\n",
        "\n",
        "        # Calculate value loss\n",
        "        # value loss is the CRITIC part of my code\n",
        "        value_loss = 0.5 * tf.square(ret - values_)\n",
        "\n",
        "        # Calculate entropy (for exploration)\n",
        "        # entropy encourages exploration by discouraging the policy to become too deterministic\n",
        "        entropy = -tf.reduce_sum(probs * log_probs)\n",
        "\n",
        "        # Accumulate losses\n",
        "        loss += policy_loss + value_loss - 0.1 * entropy\n",
        "\n",
        "        # Append metrics to stats\n",
        "        self.stats[\"mean\"][\"reward\"].append(reward)\n",
        "        self.stats[\"mean\"][\"policy\"].append(policy_loss.numpy())\n",
        "        self.stats[\"mean\"][\"value\"].append(value_loss.numpy())\n",
        "        self.stats[\"mean\"][\"entropy\"].append(entropy.numpy())\n",
        "\n",
        "        # For confidence, you need to calculate the exponent of the negative log probability\n",
        "        # of the selected action. This represents the probability of the selected action.\n",
        "        #action_probability = tf.exp(-log_action_probs)\n",
        "        action_probability = tf.exp(log_action_probs)\n",
        "        self.stats[\"mean\"][\"confidence\"].append(action_probability.numpy())\n",
        "\n",
        "\n",
        "      gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "      #for var, grad in zip(self.model.trainable_variables, gradients):\n",
        "      #  if grad is None:\n",
        "      #    print(f\"Variable with None gradient: {var.name}\")\n",
        "      #  if grad is not None:\n",
        "      #    print(f\"Variable with a gradient: {var.name}\")\n",
        "      #print()\n",
        "\n",
        "      # Apply only non-None gradients\n",
        "      gradients_to_apply = [(grad, var) for grad, var in zip(gradients, self.model.trainable_variables) if grad is not None]\n",
        "      self.optimizer.apply_gradients(gradients_to_apply)\n",
        "      #print('here6')\n",
        "      # Clear transitions and reset hidden state after each episode\n",
        "      self.transitions.clear()\n",
        "\n",
        "      if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
        "        # Log training information\n",
        "        msg = \"{:6d}. \".format(self.no_train_step)\n",
        "        msg += \"  \".join(\"{}: {: 3.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
        "        msg += \"  \" + \"  \".join(\"{}: {:2d}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
        "        msg += \"  vocab: {:3d}\".format(len(self.id2word))\n",
        "        print(msg)\n",
        "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_gradient(self, obs, score, done, infos):\n",
        "      # Get model's output - scores for each command and value estimation\n",
        "      # Build agent's observation: feedback + look + inventory.\n",
        "      input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "\n",
        "      input_tensor = self._process([input_], self.tokenizer)\n",
        "\n",
        "      commands_tensor = self._process(infos[\"admissible_commands\"], self.tokenizer)\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        outputs, indexes, value = self.model(input_tensor, commands_tensor)\n",
        "\n",
        "        returns, advantages = self._compute_advantage(value)\n",
        "        # Training logic\n",
        "        self.no_train_step += 1\n",
        "\n",
        "        if self.transitions:\n",
        "            # Calculate reward\n",
        "            reward = score - self.last_score\n",
        "            self.last_score = score\n",
        "            if infos[\"won\"]:\n",
        "                reward += 100\n",
        "            if infos[\"lost\"]:\n",
        "                reward -= 100\n",
        "            # Update the last transition with the calculated reward\n",
        "            self.transitions[-1][0] = reward\n",
        "\n",
        "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
        "          loss = 0\n",
        "          # actor critic policy gradient using advantage\n",
        "          # using advantage rather than the raw reward reduces variance\n",
        "          for transition, ret, adv in zip(self.transitions, returns, advantages):\n",
        "            #print('trans: ',self.transitions)\n",
        "            reward, indexes_, outputs_, values_ = transition\n",
        "            # Calculate policy loss\n",
        "            probs = tf.nn.softmax(outputs_)\n",
        "            log_probs = tf.math.log(probs)\n",
        "            log_action_probs = tf.reduce_sum(tf.one_hot(indexes_, outputs_.shape[-1]) * log_probs)\n",
        "\n",
        "            # Calculate policy loss\n",
        "            # this is the ACTOR part\n",
        "            policy_loss = -log_action_probs * adv\n",
        "\n",
        "            # Calculate value loss\n",
        "            # value loss is the CRITIC part of my code\n",
        "            value_loss = 0.5 * tf.square(ret - values_)\n",
        "\n",
        "            # Calculate entropy (for exploration)\n",
        "            # entropy encourages exploration by discouraging the policy to become too deterministic\n",
        "            entropy = -tf.reduce_sum(probs * log_probs)\n",
        "\n",
        "            # Accumulate losses\n",
        "            loss += policy_loss + value_loss - 0.1 * entropy\n",
        "\n",
        "            # Append metrics to stats\n",
        "            self.stats[\"mean\"][\"reward\"].append(reward)\n",
        "            self.stats[\"mean\"][\"policy\"].append(policy_loss.numpy())\n",
        "            self.stats[\"mean\"][\"value\"].append(value_loss.numpy())\n",
        "            self.stats[\"mean\"][\"entropy\"].append(entropy.numpy())\n",
        "\n",
        "            # For confidence, you need to calculate the exponent of the negative log probability\n",
        "            # of the selected action. This represents the probability of the selected action.\n",
        "            action_probability = tf.exp(-log_action_probs)\n",
        "            self.stats[\"mean\"][\"confidence\"].append(action_probability.numpy())\n",
        "\n",
        "            # Clear transitions and reset hidden state after each episode\n",
        "            self.transitions.clear()\n",
        "          else:\n",
        "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
        "            # Store transition for training\n",
        "            self.transitions.append([None, indexes, outputs, value]) # why?\n",
        "            #print('trans: ',self.transitions)\n",
        "\n",
        "      gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "      #for var, grad in zip(self.model.trainable_variables, gradients):\n",
        "      #  if grad is None:\n",
        "      #    print(f\"Variable with None gradient: {var.name}\")\n",
        "      #  if grad is not None:\n",
        "      #    print(f\"Variable with a gradient: {var.name}\")\n",
        "      #print()\n",
        "\n",
        "    def act(self, obs, score, done, infos, no_episode):\n",
        "        #print('ACT METHOD')\n",
        "        # Convert observation and commands to model input format\n",
        "\n",
        "        # Build agent's observation: feedback + look + inventory.\n",
        "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "\n",
        "        input_tensor = self._process([input_], self.tokenizer)\n",
        "\n",
        "        commands_tensor = self._process(infos[\"admissible_commands\"], self.tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "        # Get model's output - scores for each command and value estimation\n",
        "        with tf.GradientTape() as tape:\n",
        "          outputs, indexes, value = self.model(input_tensor, commands_tensor)\n",
        "\n",
        "          #tape.watch(outputs)\n",
        "          #tape.watch(indexes)\n",
        "          #tape.watch(value)\n",
        "\n",
        "          chosen_action_index = tf.squeeze(indexes).numpy()\n",
        "\n",
        "          action = infos[\"admissible_commands\"][chosen_action_index]\n",
        "\n",
        "          #print(action)\n",
        "          # test\n",
        "          if self.mode == \"test\":\n",
        "              if done:\n",
        "                  #print('DONE')\n",
        "                  self.model.reset_hidden(1)\n",
        "              return action\n",
        "\n",
        "          # Training logic\n",
        "          self.no_train_step += 1\n",
        "\n",
        "          if self.transitions:\n",
        "              # Calculate reward\n",
        "              reward = score - self.last_score\n",
        "              self.last_score = score\n",
        "              if infos[\"won\"]:\n",
        "                  reward += 100\n",
        "              if infos[\"lost\"]:\n",
        "                  reward -= 100\n",
        "              # Update the last transition with the calculated reward\n",
        "              self.transitions[-1][0] = reward\n",
        "\n",
        "          #self.transitions.append([indexes, outputs, value]) # why?\n",
        "\n",
        "          self.stats[\"max\"][\"score\"].append(score)\n",
        "\n",
        "          # Perform training at specified frequency\n",
        "\n",
        "          if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
        "\n",
        "              #print(\"IF\",self.no_train_step)\n",
        "              self._train_loop(value, tape)\n",
        "              #self._debug_train(input_tensor, commands_tensor, infos)\n",
        "\n",
        "          else:\n",
        "            #print(self.no_train_step)\n",
        "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
        "            # Store transition for training\n",
        "            self.transitions.append([None, indexes, outputs, value]) # why?\n",
        "            #print('trans: ',self.transitions)\n",
        "\n",
        "          if done:\n",
        "              self.last_score = 0\n",
        "              #self.model.reset_hidden(1)\n",
        "\n",
        "          return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iJo_3L1bRx5A"
      },
      "outputs": [],
      "source": [
        "nagent = NeuralAgent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukf7uZA1qb-X",
        "outputId": "f36d3ca0-73d9-41b1-ba04-492238ae0165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tw-rewardsDense_goalDetailed.z8.........  1000. reward:  0.039  policy:  0.475  value:  0.089  entropy:  2.330  confidence:  0.100  score:  7  vocab: 320\n",
            "..........  2000. reward:  0.036  policy:  1.276  value:  0.275  entropy:  2.326  confidence:  0.099  score:  6  vocab: 321\n",
            "..........  3000. reward:  0.041  policy:  11.334  value:  15.313  entropy:  2.325  confidence:  0.100  score:  6  vocab: 326\n",
            "..........  4000. reward:  0.047  policy:  14.794  value:  22.787  entropy:  2.391  confidence:  0.093  score:  5  vocab: 331\n",
            "..........  5000. reward:  0.056  policy:  15.551  value:  24.978  entropy:  2.394  confidence:  0.093  score:  9  vocab: 341\n",
            "..........  6000. reward:  0.048  policy:  15.793  value:  26.341  entropy:  2.371  confidence:  0.096  score:  5  vocab: 341\n",
            "..........  7000. reward:  0.044  policy:  16.337  value:  27.898  entropy:  2.384  confidence:  0.095  score:  6  vocab: 341\n",
            "..........  8000. reward:  0.031  policy:  16.054  value:  28.937  entropy:  2.295  confidence:  0.102  score:  5  vocab: 341\n",
            "..........  9000. reward:  0.044  policy:  17.413  value:  30.916  entropy:  2.411  confidence:  0.093  score:  6  vocab: 341\n",
            ".......... 10000. reward:  0.037  policy:  17.275  value:  32.138  entropy:  2.345  confidence:  0.098  score:  6  vocab: 341\n",
            "......... 11000. reward:  0.056  policy:  18.451  value:  34.394  entropy:  2.425  confidence:  0.092  score:  8  vocab: 341\n",
            ".......... 12000. reward:  0.044  policy:  18.242  value:  35.339  entropy:  2.364  confidence:  0.096  score:  6  vocab: 341\n",
            ".......... 13000. reward:  0.049  policy:  18.784  value:  37.256  entropy:  2.371  confidence:  0.096  score:  6  vocab: 341\n",
            ".......... 14000. reward:  0.052  policy:  19.051  value:  38.832  entropy:  2.356  confidence:  0.098  score:  6  vocab: 341\n",
            ".......... 15000. reward:  0.042  policy:  19.001  value:  40.360  entropy:  2.303  confidence:  0.102  score:  6  vocab: 341\n",
            ".......... 16000. reward:  0.044  policy:  19.696  value:  41.872  entropy:  2.342  confidence:  0.098  score:  5  vocab: 341\n",
            ".......... 17000. reward: -0.063  policy:  18.928  value:  56.010  entropy:  2.328  confidence:  0.100  score:  9  vocab: 357\n",
            ".......... 18000. reward:  0.037  policy:  20.349  value:  45.044  entropy:  2.332  confidence:  0.099  score:  7  vocab: 357\n",
            ".......... 19000. reward:  0.044  policy:  21.502  value:  47.047  entropy:  2.415  confidence:  0.092  score:  7  vocab: 357\n",
            ".......... 20000. reward:  0.042  policy:  21.480  value:  48.612  entropy:  2.373  confidence:  0.096  score:  6  vocab: 357\n",
            ".......... 21000. reward:  0.041  policy:  21.718  value:  50.463  entropy:  2.348  confidence:  0.098  score:  7  vocab: 357\n",
            "......... 22000. reward:  0.044  policy:  22.676  value:  52.577  entropy:  2.405  confidence:  0.093  score:  6  vocab: 357\n",
            ".......... 23000. reward:  0.038  policy:  22.141  value:  54.001  entropy:  2.317  confidence:  0.100  score:  6  vocab: 357\n",
            ".......... 24000. reward:  0.039  policy:  22.815  value:  56.083  entropy:  2.344  confidence:  0.098  score:  7  vocab: 357\n",
            ".......... 25000. reward:  0.040  policy:  22.967  value:  57.763  entropy:  2.325  confidence:  0.100  score:  5  vocab: 357\n",
            "......."
          ]
        }
      ],
      "source": [
        "nagent.train()\n",
        "play(nagent, 'games/tw-rewardsDense_goalDetailed.z8', max_step=100, nb_episodes=250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P3YDA-uvG6B"
      },
      "outputs": [],
      "source": [
        "nagent.test()\n",
        "play(nagent, 'games/tw-rewardsDense_goalDetailed.z8', max_step=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDpP4S0RXU0I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can skip this if you already downloaded the data in the prequisite section.\n",
        "\n",
        "from time import time\n",
        "agent = NeuralAgent()\n",
        "\n",
        "print(\"Training\")\n",
        "agent.train()  # Tell the agent it should update its parameters.\n",
        "starttime = time()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\", nb_episodes=250, verbose=True)  # Dense rewards game.\n",
        "\n",
        "print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
        "\n",
        "# Save the trained agent.\n",
        "import os\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "agent.save(agent, 'checkpoints/agent_trained_on_single_game.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "MUNAGxibRR8S",
        "outputId": "6f0a44aa-fd78-4eed-ae35-8072598458c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "tw-rewardsDense_goalDetailed.z8....."
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3f2baab5eb2d>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Tell the agent it should update its parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./games/tw-rewardsDense_goalDetailed.z8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Dense rewards game.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained in {:.2f} secs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e78c4887577a>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(agent, path, max_step, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnb_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_episode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# CHANGE FOR TF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m#command = agent.act(obs, done, infos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-dae79e8403b3>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, score, done, infos, no_episode)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# Get model's output - scores for each command and value estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommands_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m           \u001b[0;31m#tape.watch(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7a8adc95ad24>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, obs, commands)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#_, encoder_hidden = self.encoder_gru(embedded_obs) # stateless\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stateful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stateful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constants\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     def call(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/gru.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_lstm_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_lstm_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNTIME_UNKNOWN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             last_output, outputs, runtime, states = self._defun_gru_call(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/gru.py\u001b[0m in \u001b[0;36m_defun_gru_call\u001b[0;34m(self, inputs, initial_state, training, mask, sequence_lengths)\u001b[0m\n\u001b[1;32m    891\u001b[0m                     )\n\u001b[1;32m    892\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                     last_output, outputs, new_h, runtime = standard_gru(\n\u001b[0m\u001b[1;32m    894\u001b[0m                         \u001b[0;34m**\u001b[0m\u001b[0mnormal_gru_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/gru.py\u001b[0m in \u001b[0;36mstandard_gru\u001b[0;34m(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask, return_sequences)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m     last_output, outputs, new_states = backend.rnn(\n\u001b[0m\u001b[1;32m    995\u001b[0m         \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask, return_all_outputs)\u001b[0m\n\u001b[1;32m   5166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5168\u001b[0;31m             final_outputs = tf.compat.v1.while_loop(\n\u001b[0m\u001b[1;32m   5169\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5170\u001b[0m                 \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/while_loop.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    485\u001b[0m       loop_var_structure = nest.map_structure(type_spec.type_spec_from_value,\n\u001b[1;32m    486\u001b[0m                                               list(loop_vars))\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(time, *_)\u001b[0m\n\u001b[1;32m   5014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5015\u001b[0m         while_loop_kwargs = {\n\u001b[0;32m-> 5016\u001b[0;31m             \u001b[0;34m\"cond\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtime_steps_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5017\u001b[0m             \u001b[0;34m\"maximum_iterations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5018\u001b[0m             \u001b[0;34m\"parallel_iterations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x, y, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mless\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5252\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5253\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5254\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   5255\u001b[0m         _ctx, \"Less\", name, x, y)\n\u001b[1;32m   5256\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utZRpYPHXU2E"
      },
      "outputs": [],
      "source": [
        "# We report the score and steps averaged over 10 playthroughs.\n",
        "agent = tf.keras.models.load_model('checkpoints/agent_trained_on_single_game.pt')\n",
        "\n",
        "agent.test()\n",
        "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Dense rewards game."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1XmDT4JRXoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEwbjcwKw2jd"
      },
      "outputs": [],
      "source": [
        "rangent = RandomAgent()\n",
        "play(rangent, 'games/tw-rewardsDense_goalDetailed.z8', max_step=50, nb_episodes=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### neuralagent2 (not runnable)"
      ],
      "metadata": {
        "id": "r4jZ9lVOLCz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ldDHd2LVTP8"
      },
      "outputs": [],
      "source": [
        "# TENSORFLOW\n",
        "\n",
        "class NeuralAgent2:\n",
        "    # ... (Initialization and utility functions remain largely the same)\n",
        "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
        "    MAX_VOCAB_SIZE = 1000\n",
        "    UPDATE_FREQUENCY = 10\n",
        "    LOG_FREQUENCY = 1000\n",
        "    GAMMA = 0.9\n",
        "    LR = 0.00003\n",
        "    BATCH_SIZE=64\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, oov_token=\"<UNK>\")\n",
        "        self._initialized = False\n",
        "        self._epsiode_has_started = False\n",
        "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
        "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
        "\n",
        "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), 0.00003) # CHANGE FOR TF\n",
        "        self.optimizer = tf.optimizers.Adam(learning_rate=self.LR)\n",
        "\n",
        "        self.last_score = 0 # TODO REMOVE LATER\n",
        "        self.transitions = []\n",
        "        self.mode = \"test\"\n",
        "\n",
        "\n",
        "\n",
        "    def train(self): # CHANGE FOR TF\n",
        "        self.mode = \"train\"\n",
        "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "        self.transitions = []\n",
        "        #self.model.reset_hidden(1)\n",
        "        self.last_score = 0\n",
        "        self.no_train_step = 0\n",
        "\n",
        "    def test(self): # CHANGE FOR TF\n",
        "        self.mode = \"test\"\n",
        "        #self.model.reset_hidden(1)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> EnvInfos: # WORKING\n",
        "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
        "                        won=True, lost=True)\n",
        "\n",
        "    def _get_word_id(self, word): # WORKING\n",
        "\n",
        "        #print('GET WORD ID METHOD')\n",
        "        if word not in self.word2id:\n",
        "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
        "                return self.word2id[\"<UNK>\"]\n",
        "\n",
        "            self.id2word.append(word)\n",
        "            self.word2id[word] = len(self.word2id)\n",
        "\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def _tokenize(self, text): # WORKING\n",
        "\n",
        "        #print('TOKENIZE METHOD')\n",
        "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
        "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
        "        word_ids = list(map(self._get_word_id, text.split()))\n",
        "        return word_ids\n",
        "\n",
        "    def _process(self, texts, tokenizer): # WORKING\n",
        "\n",
        "        #print('PROCESS METHOD')\n",
        "        #tokenized_texts = tokenizer.texts_to_sequences(texts)\n",
        "        texts = list(map(self._tokenize, texts))\n",
        "        max_len = max(len(l) for l in texts)\n",
        "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            padded[i, :len(text)] = text\n",
        "        #padded_texts = tf.keras.preprocessing.sequence.pad_sequences(tokenized_texts, maxlen=max_len)\n",
        "\n",
        "        # Convert the NumPy array to a TensorFlow tensor\n",
        "        padded_tensor = tf.convert_to_tensor(padded, dtype=tf.int32)\n",
        "\n",
        "        # Transpose the tensor to switch from Batch x Sequence to Sequence x Batch\n",
        "        # padded_tensor = tf.transpose(padded_tensor, perm=[1, 0])\n",
        "\n",
        "        return padded\n",
        "\n",
        "    def _compute_advantage(self, last_values): # TF\n",
        "      returns, advantages = [], []\n",
        "      R = last_values\n",
        "      for t in reversed(range(len(self.transitions))):\n",
        "          rewards, _, _, values = self.transitions[t]\n",
        "          R = rewards + self.GAMMA * R\n",
        "          adv = R - values\n",
        "          returns.append(R)\n",
        "          advantages.append(adv)\n",
        "\n",
        "      return returns[::-1], advantages[::-1]\n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "      # Check if there are enough transitions to form a batch\n",
        "      if len(self.transitions) < self.BATCH_SIZE:\n",
        "          return\n",
        "\n",
        "      # Sample a batch of transitions\n",
        "      batch = random.sample(self.transitions, self.BATCH_SIZE)\n",
        "\n",
        "      # Separate the data into states, actions, rewards, etc.\n",
        "      states, commands, rewards, actions, outputs, values = zip(*batch)\n",
        "\n",
        "      #for state in states:\n",
        "        #print(state.shape)\n",
        "\n",
        "      # Convert lists to tensors\n",
        "      #states = tf.convert_to_tensor(np.array(states), dtype=tf.float32)\n",
        "      #commands = tf.convert_to_tensor(np.array(commands), dtype=tf.float32)\n",
        "      rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "      actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "      values = tf.convert_to_tensor(values, dtype=tf.float32)\n",
        "\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions, _, predicted_values = self.model(states, commands)\n",
        "          loss = self.compute_loss(predictions, rewards, actions, predicted_values)\n",
        "          # Compute the loss\n",
        "          # This step will depend on your specific RL algorithm.\n",
        "          # For example, in a value-based method, you might compute the TD error,\n",
        "          # while in a policy gradient method, you'd compute the policy gradient loss.\n",
        "          # Here's a placeholder for loss computation:\n",
        "          #loss = self.compute_loss(rewards, actions, outputs, values)\n",
        "\n",
        "\n",
        "      # Compute gradients and apply them\n",
        "      gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "      # Optionally, you can clear transitions after training\n",
        "      self.transitions.clear()\n",
        "\n",
        "    def compute_loss(self, predictions, rewards, actions, predicted_values):\n",
        "        # Define how you compute your loss here\n",
        "        # This will depend on your specific reinforcement learning algorithm\n",
        "        # For example, it could involve calculating the TD error for Q-learning\n",
        "        # or the policy gradient loss for policy-based methods\n",
        "        return tf.reduce_mean(tf.square(predicted_values - rewards))  # Placeholder\n",
        "\n",
        "\n",
        "    def act(self, obs, score, done, infos, no_episode):\n",
        "        # Convert observation and commands to model input format\n",
        "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "        input_tensor = self._process([input_], self.tokenizer)\n",
        "        commands_tensor = self._process(infos[\"admissible_commands\"], self.tokenizer)\n",
        "\n",
        "        # Forward pass through the model to get scores for each command\n",
        "        outputs, indexes, value = self.model(input_tensor, commands_tensor)\n",
        "\n",
        "        # Select action based on the model's output\n",
        "        chosen_action_index = tf.squeeze(indexes).numpy()\n",
        "        action = infos[\"admissible_commands\"][chosen_action_index]\n",
        "\n",
        "        # Calculate immediate reward and store transition\n",
        "        reward = score - self.last_score\n",
        "        self.last_score = score if not done else 0\n",
        "\n",
        "        self.transitions.append((input_tensor, commands_tensor, reward, indexes, outputs, value))\n",
        "\n",
        "        return action\n",
        "\n",
        "    def act_old(self, obs, score, done, infos, no_episode):\n",
        "        #print('ACT METHOD')\n",
        "        # Convert observation and commands to model input format\n",
        "\n",
        "        # Build agent's observation: feedback + look + inventory.\n",
        "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "\n",
        "        input_tensor = self._process([input_], self.tokenizer)\n",
        "        commands_tensor = self._process(infos[\"admissible_commands\"], self.tokenizer)\n",
        "        # Get model's output - scores for each command and value estimation\n",
        "        outputs, indexes, value = self.model(input_tensor, commands_tensor)\n",
        "        chosen_action_index = tf.squeeze(indexes).numpy()\n",
        "\n",
        "\n",
        "        action = infos[\"admissible_commands\"][chosen_action_index]\n",
        "\n",
        "        #print(action)\n",
        "        # test\n",
        "        if self.mode == \"test\":\n",
        "            if done:\n",
        "                print('DONE')\n",
        "                #self.model.reset_hidden(1)\n",
        "            return action\n",
        "\n",
        "        # Training logic\n",
        "        self.no_train_step += 1\n",
        "\n",
        "        if self.transitions:\n",
        "            # Calculate reward\n",
        "            reward = score - self.last_score\n",
        "            self.last_score = score\n",
        "            if infos[\"won\"]:\n",
        "                reward += 100\n",
        "            if infos[\"lost\"]:\n",
        "                reward -= 100\n",
        "            # Update the last transition with the calculated reward\n",
        "            self.transitions[-1][0] = reward\n",
        "\n",
        "\n",
        "\n",
        "        #self.transitions.append([indexes, outputs, value]) # why?\n",
        "\n",
        "        self.stats[\"max\"][\"score\"].append(score)\n",
        "\n",
        "        # Perform training at specified frequency\n",
        "\n",
        "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
        "\n",
        "            #print(\"IF\",self.no_train_step)\n",
        "            self._train_loop(value)\n",
        "            #self._debug_train(input_tensor, commands_tensor, infos)\n",
        "\n",
        "        else:\n",
        "          #print(self.no_train_step)\n",
        "          # Keep information about transitions for Truncated Backpropagation Through Time.\n",
        "          # Store transition for training\n",
        "          self.transitions.append([None, indexes, outputs, value]) # why?\n",
        "          #print('trans: ',self.transitions)\n",
        "\n",
        "        if done:\n",
        "            self.last_score = 0\n",
        "            #self.model.reset_hidden(1)\n",
        "\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw3fPdxwgwpk"
      },
      "outputs": [],
      "source": [
        "nagent = NeuralAgent2()\n",
        "#nagent.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nE_zp55U_hL"
      },
      "outputs": [],
      "source": [
        "#nagent = NeuralAgent2()\n",
        "#nagent.train()\n",
        "play(nagent, 'games/tw-rewardsDense_goalDetailed.z8', max_step=50, nb_episodes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbcnBkqWU_j9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VbO-8D4uJul"
      },
      "source": [
        "### gpt-4 attempt using function approximation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrVtHb8OQVvI"
      },
      "outputs": [],
      "source": [
        "class EPSdecay:\n",
        "    def __init__(self,min_epsilon=0.01,max_epsilon=1.0,decay_rate=0.01):\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.decay_rate = decay_rate\n",
        "    def __call__(self,episode)->float:\n",
        "        return self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay_rate*episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz4xixGym7Yq"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(descriptions):\n",
        "    word_counts = Counter(word for desc in descriptions for word in desc.split())\n",
        "    vocabulary = {word: i for i, word in enumerate(word_counts.keys())}\n",
        "    return vocabulary\n",
        "\n",
        "# Example descriptions from your text-based game\n",
        "descriptions = [\n",
        "    \"You see a key and a door\",\n",
        "    \"You are in a dark room\",\n",
        "    # ... more descriptions from your game\n",
        "]\n",
        "\n",
        "vocabulary = build_vocabulary(descriptions)\n",
        "\n",
        "def preprocess_state(description, vocabulary):\n",
        "    state_vector = np.zeros(len(vocabulary))\n",
        "    for word in description.split():\n",
        "        if word in vocabulary:\n",
        "            state_vector[vocabulary[word]] += 1\n",
        "    return state_vector\n",
        "\n",
        "# Example usage\n",
        "state_description = \"You are in a room with a key\"\n",
        "state_vector = preprocess_state(state_description, vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMFlo6jJuyLR"
      },
      "outputs": [],
      "source": [
        "def build_model(input_size, output_size):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(input_size,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(output_size)\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NACIOKbquyNv"
      },
      "outputs": [],
      "source": [
        "def train_step(model, optimizer, state, action, reward, next_state, done):\n",
        "    # Predict Q-values for the current state\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_values = model(state)\n",
        "        q_action = tf.reduce_sum(tf.one_hot(action, action_space) * q_values, axis=1)\n",
        "\n",
        "        # Predict the Q-values for next state\n",
        "        q_values_next = model(next_state)\n",
        "        q_next = tf.reduce_max(q_values_next, axis=1)\n",
        "        #bellman equation\n",
        "        q_target = reward + (1 - done) * discount_factor * q_next\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = tf.reduce_mean(tf.square(q_target - q_action))\n",
        "\n",
        "    # Backpropagate the error\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2s0uqlS4uE4"
      },
      "outputs": [],
      "source": [
        "discount_factor = 0.95\n",
        "num_episodes = 20\n",
        "learning_rate = 0.001\n",
        "#epsilon = 0.5 # use fixed epsilon\n",
        "epsilon = EPSdecay()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoNF-8UlvaEw"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "model = build_model(len(state_vector), action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag-bTLJEuyp5",
        "outputId": "483841b4-b944-47cf-ad60-d7fc041f3c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg moves 20.5\n",
            "avg score 0.03\n",
            "avg normalized score 0.03\n",
            "avg moves 23.27777777777778\n",
            "avg score 0.01\n",
            "avg normalized score 0.01\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-283-fe6c422b25ab>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m#print(next_state_vector)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m# Perform training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-116-94c728c2cf3f>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Predict Q-values for the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mq_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \"\"\"\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autographed_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0m\u001b[1;32m   1128\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 object_name=(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36minject_argument_info_in_traceback\u001b[0;34m(fn, object_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdecorator_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mdecorator_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   decorator = TFDecorator(decorator_name, target, decorator_doc,\n\u001b[0m\u001b[1;32m    137\u001b[0m                           decorator_argspec)\n\u001b[1;32m    138\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorator_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tf_decorator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, decorator_name, target, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Certain callables such as builtins can not be inspected for signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3254\u001b[0;31m     return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n\u001b[0m\u001b[1;32m   3255\u001b[0m                                    globals=globals, locals=locals, eval_str=eval_str)\n\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3000\u001b[0m                       follow_wrapped=True, globals=None, locals=None, eval_str=False):\n\u001b[1;32m   3001\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0m\u001b[1;32m   3003\u001b[0m                                         \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \"\"\"\n\u001b[1;32m   2386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2387\u001b[0;31m     _get_signature_of = functools.partial(_signature_from_callable,\n\u001b[0m\u001b[1;32m   2388\u001b[0m                                 \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2389\u001b[0m                                 \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Collect some statistics: nb_steps, final reward.\n",
        "avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
        "for episode in range(num_episodes):\n",
        "    state, infos = env.reset()\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    total_steps = 0\n",
        "    while not done:\n",
        "      #print('episode: ', episode)\n",
        "\n",
        "      state_vector = preprocess_state(state, vocabulary)\n",
        "      state_vector = np.expand_dims(state_vector, axis=0) # Convert from shape (features,) to (1, features)\n",
        "\n",
        "      # Select action using epsilon-greedy policy\n",
        "      if random.uniform(0,1) > epsilon(episode):\n",
        "        q_values = model.predict(state_vector, verbose=0)[0]\n",
        "        action_ind = np.argmax(q_values)\n",
        "        print(q_values)\n",
        "      else:\n",
        "        action_ind = random.randrange(len(infos[\"admissible_commands\"]))\n",
        "        #action = infos[\"admissible_commands\"][action_ind] # random action for now\n",
        "\n",
        "      action = infos[\"admissible_commands\"][action_ind]\n",
        "      #print(action)\n",
        "      next_state, reward, done, infos = env.step(action)\n",
        "      total_steps += 1\n",
        "      total_rewards += reward\n",
        "\n",
        "      # Preprocess states if required\n",
        "      next_state_vector = preprocess_state(next_state, vocabulary)\n",
        "      next_state_vector = np.expand_dims(next_state_vector, axis=0) # Convert from shape (features,) to (1, features)\n",
        "\n",
        "      #print(next_state_vector)\n",
        "      # Perform training step\n",
        "      train_step(model, optimizer, state_vector, action_ind, reward, next_state_vector, done)\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "      avg_moves.append(total_steps)\n",
        "      avg_scores.append(reward)\n",
        "      avg_norm_scores.append(reward / infos[\"max_score\"])\n",
        "    #print('total steps', total_steps)\n",
        "    #print('total rewards', total_rewards)\n",
        "    print('avg moves', np.mean(avg_moves))\n",
        "    print('avg score', \"{:.2f}\".format(np.mean(avg_scores)))\n",
        "    print('avg normalized score', \"{:.2f}\".format(np.mean(avg_norm_scores)))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "T1onOUgXTEFM",
        "2VbO-8D4uJul"
      ],
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMC5riz/o/zp9ndRtsj/POJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}