{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathanjander/Best-README-Template/blob/master/textworld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KygCLbPDOPQ"
      },
      "source": [
        "# Textworld example\n",
        "\n",
        "## notes\n",
        "- [official pytorch example](https://colab.research.google.com/github/microsoft/TextWorld/blob/main/notebooks/Building%20a%20simple%20agent.ipynb#scrollTo=ChrM9GGGlrtf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOK2hjFEC8tx",
        "outputId": "d9d0970e-3b89-4f80-9574-7cc3e253cdf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textworld\n",
            "  Downloading textworld-1.6.1.tar.gz (708 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.6/708.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from textworld) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.17.1 in /usr/local/lib/python3.10/dist-packages (from textworld) (4.66.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from textworld) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.10/dist-packages (from textworld) (3.2.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textworld) (10.1.0)\n",
            "Collecting tatsu>=5.8.3 (from textworld)\n",
            "  Downloading TatSu-5.8.3-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hashids>=1.2.0 (from textworld)\n",
            "  Downloading hashids-1.3.1-py2.py3-none-any.whl (6.6 kB)\n",
            "Collecting jericho>=3.0.3 (from textworld)\n",
            "  Downloading jericho-3.1.2.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mementos>=1.3.1 (from textworld)\n",
            "  Downloading mementos-1.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from textworld) (2.4.0)\n",
            "Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.10/dist-packages (from textworld) (3.0.43)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->textworld) (2.21)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from jericho>=3.0.3->textworld) (3.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit->textworld) (0.2.12)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.0->jericho>=3.0.3->textworld) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.1.0->jericho>=3.0.3->textworld) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.1.0->jericho>=3.0.3->textworld) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.1.0->jericho>=3.0.3->textworld) (2.1.3)\n",
            "Building wheels for collected packages: textworld, jericho\n",
            "  Building wheel for textworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textworld: filename=textworld-1.6.1-cp310-cp310-linux_x86_64.whl size=6723030 sha256=eef507ee2aef58cc8c14e4353d3f8e3d72893d0552e0e932faddab06dcab360a\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/ff/fd/806c0a27e57b83e5aa6eae2dcaf74f93afca93936bfeecbf6e\n",
            "  Building wheel for jericho (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jericho: filename=jericho-3.1.2-py3-none-any.whl size=325097 sha256=4ac9cdfe20487a039ffa09cfaec9eb50583e00aff5efa4457fedf6f80b8352c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1d/a7/91e11767b583fe77fae27d292e724d0dc8cd4335dab886adfe\n",
            "Successfully built textworld jericho\n",
            "Installing collected packages: mementos, tatsu, hashids, jericho, textworld\n",
            "Successfully installed hashids-1.3.1 jericho-3.1.2 mementos-1.3.1 tatsu-5.8.3 textworld-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install textworld\n",
        "#!pip install gym\n",
        "#!pip install gym==0.21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D6GjCzJMEM-i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from glob import glob\n",
        "import os\n",
        "import re\n",
        "from typing import List, Mapping, Any, Optional\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import textworld\n",
        "import textworld.gym\n",
        "from textworld import EnvInfos\n",
        "from time import time\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j43nOrbzGOLx"
      },
      "source": [
        "## init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BajFp_UAGXr5",
        "outputId": "eda289da-a245-4a0b-94b9-db04a30a4617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed: 42\n",
            "Game generated: /content/games/tw-rewardsDense_goalDetailed.z8\n",
            "\n",
            "Objective:\n",
            "I hope you're ready to go into rooms and interact with objects, because you've just entered TextWorld! Here is how to play! First stop, open the chest drawer. After that, recover the old key from the chest drawer within the bedroom. Then, check that the wooden door is unlocked with the old key. And then, open the wooden door inside the bedroom. Then, go to the east. And then, look and see that the screen door in the kitchen is ajar. After that, make an effort to venture east. And then, attempt to move south. With that accomplished, lift the bell pepper from the floor of the garden. After that, move north. Okay, and then, attempt to take a trip west. Following that, rest the bell pepper on the stove. And once you've done that, you win!\n",
            "\n",
            "Walkthrough:\n",
            "open chest drawer > take old key from chest drawer > unlock wooden door with old key > open wooden door > go east > open screen door > go east > go south > take bell pepper > go north > go west > put bell pepper on stove\n",
            "\n",
            "-= Stats =-\n",
            "Nb. locations: 6\n",
            "Nb. objects: 28\n"
          ]
        }
      ],
      "source": [
        "#!tw-make custom --world-size 2 --quest-length 4 --nb-objects 10 --output tw_games/game.ulx -f -v --seed 123\n",
        "!tw-make tw-simple --rewards dense  --goal detailed --seed 42 --output games/tw-rewardsDense_goalDetailed.z8 -v -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2mYslX3VGMVb"
      },
      "outputs": [],
      "source": [
        "request_infos = textworld.EnvInfos(\n",
        "    admissible_commands=True,  # All commands relevant to the current state.\n",
        "    entities=True,             # List of all interactable entities found in the game.\n",
        "    facts=True,  # All the facts that are currently true about the world.\n",
        "    intermediate_reward=True,\n",
        "    max_score = True,\n",
        "    inventory=True,\n",
        "    description=True,\n",
        "    command_templates = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vnu5jFOQGMYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3751f4-040a-4431-cd8f-91099cc0c986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities: ['wooden door', 'screen door', 'chest drawer', 'antique trunk', 'refrigerator', 'toilet', 'bath', 'lettuce', 'bell pepper', 'apple', 'shovel', 'king-size bed', 'counter', 'set of chairs', 'stove', 'kitchen island', 'sink', 'couch', 'low table', 'tv', 'bbq', 'patio table', 'tomato plant', 'half of a bag of chips', 'milk', 'old key', 'soap bar', 'toothbrush', 'remote', 'note', 'north', 'south', 'east', 'west']\n",
            "\n",
            "Admissible commands:\n",
            "  examine antique trunk\n",
            "  examine chest drawer\n",
            "  examine king-size bed\n",
            "  examine wooden door\n",
            "  inventory\n",
            "  look\n",
            "  open antique trunk\n",
            "  open chest drawer\n"
          ]
        }
      ],
      "source": [
        "# Requesting additional information should be done when registering the game.\n",
        "#env_id = textworld.gym.register_game('tw_games/game.ulx', request_infos)\n",
        "env_id = textworld.gym.register_game(\"./games/tw-rewardsDense_goalDetailed.z8\", request_infos)\n",
        "env = textworld.gym.make(env_id)\n",
        "\n",
        "obs, infos = env.reset()\n",
        "print(\"Entities: {}\\n\".format(infos[\"entities\"]))\n",
        "print(\"Admissible commands:\\n  {}\".format(\"\\n  \".join(infos[\"admissible_commands\"])))\n",
        "#print(\"command_templates:\\n  {}\".format(\"\\n  \".join(infos[\"command_templates\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "11ARgwqTvguA"
      },
      "outputs": [],
      "source": [
        "action_space = len(infos[\"admissible_commands\"])\n",
        "state_space = 20 # number of rooms times number of items 2*10 (dont think this is correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### playing the game"
      ],
      "metadata": {
        "id": "T1onOUgXTEFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    done = False\n",
        "    obs, _ = env.reset()\n",
        "    print(obs)\n",
        "\n",
        "    print(infos[\"admissible_commands\"])\n",
        "    nb_moves = 0\n",
        "    while not done:\n",
        "        command = input(\"> \")\n",
        "        obs, score, done, infos = env.step(command)\n",
        "        print(obs)\n",
        "        print('Score',score)\n",
        "        print(infos[\"admissible_commands\"])\n",
        "        nb_moves += 1\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass  # Press the stop button in the toolbar to quit the game.\n",
        "\n",
        "print(\"Played {} steps, scoring {} points.\".format(nb_moves, score))"
      ],
      "metadata": {
        "id": "xxs7s-XJNVTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VbO-8D4uJul"
      },
      "source": [
        "### gpt-4 attempt using function approximation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "RrVtHb8OQVvI"
      },
      "outputs": [],
      "source": [
        "class EPSdecay:\n",
        "    def __init__(self,min_epsilon=0.01,max_epsilon=1.0,decay_rate=0.01):\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.decay_rate = decay_rate\n",
        "    def __call__(self,episode)->float:\n",
        "        return self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay_rate*episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "yz4xixGym7Yq"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(descriptions):\n",
        "    word_counts = Counter(word for desc in descriptions for word in desc.split())\n",
        "    vocabulary = {word: i for i, word in enumerate(word_counts.keys())}\n",
        "    return vocabulary\n",
        "\n",
        "# Example descriptions from your text-based game\n",
        "descriptions = [\n",
        "    \"You see a key and a door\",\n",
        "    \"You are in a dark room\",\n",
        "    # ... more descriptions from your game\n",
        "]\n",
        "\n",
        "vocabulary = build_vocabulary(descriptions)\n",
        "\n",
        "def preprocess_state(description, vocabulary):\n",
        "    state_vector = np.zeros(len(vocabulary))\n",
        "    for word in description.split():\n",
        "        if word in vocabulary:\n",
        "            state_vector[vocabulary[word]] += 1\n",
        "    return state_vector\n",
        "\n",
        "# Example usage\n",
        "state_description = \"You are in a room with a key\"\n",
        "state_vector = preprocess_state(state_description, vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "vMFlo6jJuyLR"
      },
      "outputs": [],
      "source": [
        "def build_model(input_size, output_size):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(input_size,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(output_size)\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "NACIOKbquyNv"
      },
      "outputs": [],
      "source": [
        "def train_step(model, optimizer, state, action, reward, next_state, done):\n",
        "    # Predict Q-values for the current state\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_values = model(state)\n",
        "        q_action = tf.reduce_sum(tf.one_hot(action, action_space) * q_values, axis=1)\n",
        "\n",
        "        # Predict the Q-values for next state\n",
        "        q_values_next = model(next_state)\n",
        "        q_next = tf.reduce_max(q_values_next, axis=1)\n",
        "        #bellman equation\n",
        "        q_target = reward + (1 - done) * discount_factor * q_next\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = tf.reduce_mean(tf.square(q_target - q_action))\n",
        "\n",
        "    # Backpropagate the error\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discount_factor = 0.95\n",
        "num_episodes = 20\n",
        "learning_rate = 0.001\n",
        "#epsilon = 0.5 # use fixed epsilon\n",
        "epsilon = EPSdecay()"
      ],
      "metadata": {
        "id": "S2s0uqlS4uE4"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "hoNF-8UlvaEw"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "model = build_model(len(state_vector), action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Ag-bTLJEuyp5",
        "outputId": "483841b4-b944-47cf-ad60-d7fc041f3c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg moves 20.5\n",
            "avg score 0.03\n",
            "avg normalized score 0.03\n",
            "avg moves 23.27777777777778\n",
            "avg score 0.01\n",
            "avg normalized score 0.01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-283-fe6c422b25ab>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m#print(next_state_vector)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m# Perform training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-116-94c728c2cf3f>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Predict Q-values for the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mq_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \"\"\"\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autographed_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0m\u001b[1;32m   1128\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 object_name=(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36minject_argument_info_in_traceback\u001b[0;34m(fn, object_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdecorator_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mdecorator_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   decorator = TFDecorator(decorator_name, target, decorator_doc,\n\u001b[0m\u001b[1;32m    137\u001b[0m                           decorator_argspec)\n\u001b[1;32m    138\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorator_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tf_decorator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, decorator_name, target, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Certain callables such as builtins can not be inspected for signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3254\u001b[0;31m     return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n\u001b[0m\u001b[1;32m   3255\u001b[0m                                    globals=globals, locals=locals, eval_str=eval_str)\n\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3000\u001b[0m                       follow_wrapped=True, globals=None, locals=None, eval_str=False):\n\u001b[1;32m   3001\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0m\u001b[1;32m   3003\u001b[0m                                         \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \"\"\"\n\u001b[1;32m   2386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2387\u001b[0;31m     _get_signature_of = functools.partial(_signature_from_callable,\n\u001b[0m\u001b[1;32m   2388\u001b[0m                                 \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2389\u001b[0m                                 \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Collect some statistics: nb_steps, final reward.\n",
        "avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
        "for episode in range(num_episodes):\n",
        "    state, infos = env.reset()\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    total_steps = 0\n",
        "    while not done:\n",
        "      #print('episode: ', episode)\n",
        "\n",
        "      state_vector = preprocess_state(state, vocabulary)\n",
        "      state_vector = np.expand_dims(state_vector, axis=0) # Convert from shape (features,) to (1, features)\n",
        "\n",
        "      # Select action using epsilon-greedy policy\n",
        "      if random.uniform(0,1) > epsilon(episode):\n",
        "        q_values = model.predict(state_vector, verbose=0)[0]\n",
        "        action_ind = np.argmax(q_values)\n",
        "        print(q_values)\n",
        "      else:\n",
        "        action_ind = random.randrange(len(infos[\"admissible_commands\"]))\n",
        "        #action = infos[\"admissible_commands\"][action_ind] # random action for now\n",
        "\n",
        "      action = infos[\"admissible_commands\"][action_ind]\n",
        "      #print(action)\n",
        "      next_state, reward, done, infos = env.step(action)\n",
        "      total_steps += 1\n",
        "      total_rewards += reward\n",
        "\n",
        "      # Preprocess states if required\n",
        "      next_state_vector = preprocess_state(next_state, vocabulary)\n",
        "      next_state_vector = np.expand_dims(next_state_vector, axis=0) # Convert from shape (features,) to (1, features)\n",
        "\n",
        "      #print(next_state_vector)\n",
        "      # Perform training step\n",
        "      train_step(model, optimizer, state_vector, action_ind, reward, next_state_vector, done)\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "      avg_moves.append(total_steps)\n",
        "      avg_scores.append(reward)\n",
        "      avg_norm_scores.append(reward / infos[\"max_score\"])\n",
        "    #print('total steps', total_steps)\n",
        "    #print('total rewards', total_rewards)\n",
        "    print('avg moves', np.mean(avg_moves))\n",
        "    print('avg score', \"{:.2f}\".format(np.mean(avg_scores)))\n",
        "    print('avg normalized score', \"{:.2f}\".format(np.mean(avg_norm_scores)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-0pHpZ0vWyG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gv0kbzFeW6Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "01PO_5MJW6DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trying to implement the example code to tensorflow (my implementation)\n",
        "- using a GRU\n",
        "- using embedding"
      ],
      "metadata": {
        "id": "oirKK-onj_5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building a random baseline\n",
        "class RandomAgent(textworld.gym.Agent):\n",
        "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
        "    def __init__(self, seed=1234):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.RandomState(self.seed)\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> textworld.EnvInfos:\n",
        "        return textworld.EnvInfos(admissible_commands=True)\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
        "        return self.rng.choice(infos[\"admissible_commands\"])"
      ],
      "metadata": {
        "id": "U-bTN11fkJhb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# play function\n",
        "def play(agent, path, max_step=50, nb_episodes=10, verbose=True):\n",
        "    # torch.manual_seed(20211021)  # For reproducibility when using action sampling.\n",
        "\n",
        "    infos_to_request = agent.infos_to_request\n",
        "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
        "\n",
        "    gamefiles = [path]\n",
        "    if os.path.isdir(path):\n",
        "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
        "\n",
        "    env_id = textworld.gym.register_games(gamefiles,\n",
        "                                          request_infos=infos_to_request,\n",
        "                                          max_episode_steps=max_step)\n",
        "    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            print(os.path.dirname(path), end=\"\")\n",
        "        else:\n",
        "            print(os.path.basename(path), end=\"\")\n",
        "\n",
        "    # Collect some statistics: nb_steps, final reward.\n",
        "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
        "    for no_episode in range(nb_episodes):\n",
        "        obs, infos = env.reset()  # Start new episode.\n",
        "\n",
        "        score = 0\n",
        "        done = False\n",
        "        nb_moves = 0\n",
        "        while not done:\n",
        "            command = agent.act(obs, score, done, infos) # CHANGE FOR TF\n",
        "            #command = agent.act(obs, done, infos)\n",
        "            obs, score, done, infos = env.step(command)\n",
        "            nb_moves += 1\n",
        "\n",
        "        agent.act(obs, score, done, infos)  # Let the agent know the game is done. CHANGE FOR TF\n",
        "        # agent.act(obs, done, infos)  # Let the agent know the game is done. CHANGE FOR TF\n",
        "\n",
        "        if verbose:\n",
        "            print(\".\", end=\"\")\n",
        "        avg_moves.append(nb_moves)\n",
        "        avg_scores.append(score)\n",
        "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
        "\n",
        "    env.close()\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
        "        else:\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))"
      ],
      "metadata": {
        "id": "92ElBvEpkJkm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#agent = RandomAgent()\n",
        "#play(agent, 'tw_games/game.ulx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "yT1Q0fHTmcOP",
        "outputId": "ec134510-69eb-409a-fdb6-78b08961a9c6"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-06d8d3c5f310>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tw_games/game.ulx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomAgent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwVORzOtkJnl"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TENSORFLOW\n",
        "# TODO IMPLEMENT THE CRITIC\n",
        "# TODO INCREASE PERFORMANCE\n",
        "class CommandScorer(models.Model):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CommandScorer, self).__init__()\n",
        "        self.embedding = layers.Embedding(input_size, hidden_size)\n",
        "        self.encoder_gru = layers.GRU(hidden_size, return_sequences=True, return_state=True)\n",
        "        self.cmd_encoder_gru = layers.GRU(hidden_size, return_state=True)\n",
        "        self.state_gru = layers.GRU(hidden_size, return_state=True)\n",
        "        self.critic = layers.Dense(1) # Critic for state value estimation\n",
        "        self.att_cmd = layers.Dense(1) # Attention mechanism for commands\n",
        "\n",
        "    def call(self, obs, commands):\n",
        "        # Process observation\n",
        "        embedded_obs = self.embedding(obs)\n",
        "        _, encoder_hidden = self.encoder_gru(embedded_obs)\n",
        "\n",
        "        # Expand dimensions of encoder_hidden to fit GRU input requirements\n",
        "        # for critic state value prediction\n",
        "        encoder_hidden_expanded = tf.expand_dims(encoder_hidden, axis=1)\n",
        "        state_output, _ = self.state_gru(encoder_hidden_expanded)\n",
        "        value = self.critic(state_output)\n",
        "\n",
        "        # Process commands\n",
        "        cmds_embedding = self.embedding(commands)  # Shape: (num_commands, cmd_length, hidden_size)\n",
        "        cmd_length = cmds_embedding.shape[1]\n",
        "\n",
        "        # Reshape for batch processing\n",
        "        cmds_embedding_reshaped = tf.reshape(cmds_embedding, (-1, cmd_length, self.embedding.output_dim))\n",
        "        batch_size = tf.shape(cmds_embedding_reshaped)[0]\n",
        "\n",
        "        # Process each command as a separate sequence\n",
        "        _, cmds_encoding_last_states = self.cmd_encoder_gru(cmds_embedding_reshaped, initial_state=tf.zeros((batch_size, self.encoder_gru.units)))\n",
        "\n",
        "        # Reshape to get separate encodings for each command\n",
        "        cmds_encoding_last_states = tf.reshape(cmds_encoding_last_states, (1, -1, self.encoder_gru.units))\n",
        "\n",
        "        # Prepare state representation\n",
        "        state_hidden_repeated = tf.repeat(tf.expand_dims(encoder_hidden, axis=1), repeats=tf.shape(commands)[0], axis=1)\n",
        "\n",
        "        # Concatenate state and command encodings\n",
        "        cmd_selector_input = tf.concat([state_hidden_repeated, cmds_encoding_last_states], axis=-1)\n",
        "\n",
        "        # Compute scores and select action\n",
        "        scores = self.att_cmd(cmd_selector_input)\n",
        "        scores = tf.squeeze(scores, axis=-1)\n",
        "\n",
        "        # Calculate probabilities and sample an action\n",
        "        probs = tf.nn.softmax(scores, axis=1)\n",
        "        index = tf.random.categorical(tf.math.log(probs), num_samples=1)\n",
        "\n",
        "        return scores, index, value\n",
        "\n",
        "\n",
        "\n",
        "    def call_old(self, obs, commands):\n",
        "        embedded = self.embedding(obs)\n",
        "        _, encoder_hidden = self.encoder_gru(embedded)\n",
        "\n",
        "        cmds_embedding = self.embedding(commands) # (8, 3, 128)\n",
        "        cmds_encoding_last_states_list = []\n",
        "\n",
        "        # Process each command independently\n",
        "        for i in range(cmds_embedding.shape[0]):  # Iterate over each command\n",
        "            # Treat each command as a single time-step sequence\n",
        "            cmd = tf.expand_dims(cmds_embedding[i], axis=0)  # Shape: (1, 3, 128)\n",
        "            _, cmd_encoding = self.cmd_encoder_gru(cmd)  # Shape: (1, 128)\n",
        "            cmd_encoding = tf.reshape(cmd_encoding, (1, 1, -1))\n",
        "            cmds_encoding_last_states_list.append(cmd_encoding)\n",
        "\n",
        "\n",
        "        cmds_encoding_last_states = tf.concat(cmds_encoding_last_states_list, axis=1)  # Shape: (1, 8, 128)\n",
        "\n",
        "        state_hidden_repeated = tf.repeat(tf.expand_dims(encoder_hidden, axis=1), repeats=tf.shape(commands)[0] , axis=1)  # Shape: (1, 8, 128)\n",
        "\n",
        "        cmd_selector_input = tf.concat([state_hidden_repeated, cmds_encoding_last_states], axis=-1)  # Shape: (1, 1, 8, 256)\n",
        "\n",
        "\n",
        "        scores = self.att_cmd(cmd_selector_input) # DYNAMIC ATTEMPT\n",
        "        scores = tf.squeeze(scores, axis=-1)\n",
        "\n",
        "        # Calculate probabilities and sample an action\n",
        "        probs = tf.nn.softmax(scores, axis=1)\n",
        "        index = tf.random.categorical(tf.math.log(probs), num_samples=1)  # Use log probabilities for stability\n",
        "\n",
        "        return scores, index, value\n"
      ],
      "metadata": {
        "id": "VihokiidmNp8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TENSORFLOW\n",
        "\n",
        "class NeuralAgent:\n",
        "    # ... (Initialization and utility functions remain largely the same)\n",
        "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
        "    MAX_VOCAB_SIZE = 1000\n",
        "    UPDATE_FREQUENCY = 10\n",
        "    LOG_FREQUENCY = 1000\n",
        "    GAMMA = 0.9\n",
        "    LR = 0.001\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, oov_token=\"<UNK>\")\n",
        "        self._initialized = False\n",
        "        self._epsiode_has_started = False\n",
        "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
        "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
        "\n",
        "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), 0.00003) # CHANGE FOR TF\n",
        "        self.optimizer = tf.optimizers.Adam(learning_rate=self.LR)\n",
        "        self.mode = \"test\"\n",
        "\n",
        "\n",
        "    def train(self): # CHANGE FOR TF\n",
        "        self.mode = \"train\"\n",
        "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "        self.transitions = []\n",
        "        #self.model.reset_hidden(1)\n",
        "        self.last_score = 0\n",
        "        self.no_train_step = 0\n",
        "\n",
        "    def test(self): # CHANGE FOR TF\n",
        "        self.mode = \"test\"\n",
        "        #self.model.reset_hidden(1)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> EnvInfos: # WORKING\n",
        "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
        "                        won=True, lost=True)\n",
        "\n",
        "    def _get_word_id(self, word): # WORKING\n",
        "\n",
        "        #print('GET WORD ID METHOD')\n",
        "        if word not in self.word2id:\n",
        "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
        "                return self.word2id[\"<UNK>\"]\n",
        "\n",
        "            self.id2word.append(word)\n",
        "            self.word2id[word] = len(self.word2id)\n",
        "\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def _tokenize(self, text): # WORKING\n",
        "\n",
        "        #print('TOKENIZE METHOD')\n",
        "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
        "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
        "        word_ids = list(map(self._get_word_id, text.split()))\n",
        "        return word_ids\n",
        "\n",
        "    def _process(self, texts, tokenizer): # WORKING\n",
        "\n",
        "        #print('PROCESS METHOD')\n",
        "        #tokenized_texts = tokenizer.texts_to_sequences(texts)\n",
        "        texts = list(map(self._tokenize, texts))\n",
        "        max_len = max(len(l) for l in texts)\n",
        "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
        "        #padded_texts = tf.keras.preprocessing.sequence.pad_sequences(tokenized_texts, maxlen=max_len)\n",
        "        return padded\n",
        "\n",
        "    def _discount_rewards(self, last_values): # CHANGE FOR TF\n",
        "        returns, advantages = [], []\n",
        "        R = last_values.data\n",
        "        for t in reversed(range(len(self.transitions))):\n",
        "            rewards, _, _, values = self.transitions[t]\n",
        "            R = rewards + self.GAMMA * R\n",
        "            adv = R - values\n",
        "            returns.append(R)\n",
        "            advantages.append(adv)\n",
        "\n",
        "        return returns[::-1], advantages[::-1]\n",
        "\n",
        "    def _train_loop(self):\n",
        "      print('TRAIN_MODEL METHOD')\n",
        "\n",
        "      if not self.transitions:\n",
        "          return\n",
        "\n",
        "      # Unpack the transitions\n",
        "      rewards, actions, q_values, values = zip(*self.transitions)\n",
        "\n",
        "\n",
        "      # Convert lists to TensorFlow tensors\n",
        "      rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "      actions = tf.stack(actions)  # Assuming actions are already tensors\n",
        "      q_values = tf.stack(q_values)\n",
        "      values = tf.stack(values)\n",
        "\n",
        "      # Compute target Q-values\n",
        "      target_q_values = rewards + self.GAMMA * tf.reduce_max(q_values, axis=2)\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          # Compute the loss\n",
        "          # This part depends on your network's output and the specific algorithm (Q-learning, SARSA, etc.)\n",
        "          # Here's a basic implementation assuming Q-learning:\n",
        "\n",
        "          # Gather Q-values of taken actions\n",
        "          action_q_values = tf.reduce_sum(tf.one_hot(actions, depth=self.num_actions) * q_values, axis=2)\n",
        "          loss = tf.reduce_mean(tf.square(target_q_values - action_q_values))\n",
        "\n",
        "      # Compute gradients\n",
        "      gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "      # Apply gradients\n",
        "      self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "      # Clear transitions\n",
        "      self.transitions.clear()\n",
        "\n",
        "    def act(self, obs, score, done, infos):\n",
        "        #print('ACT METHOD')\n",
        "        # Convert observation and commands to model input format\n",
        "\n",
        "        # Build agent's observation: feedback + look + inventory.\n",
        "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "\n",
        "        input_tensor = self._process([input_], self.tokenizer)\n",
        "\n",
        "        commands_tensor = self._process(infos[\"admissible_commands\"], self.tokenizer)\n",
        "\n",
        "        # Get model's output - scores for each command and value estimation\n",
        "        outputs, indexes, value = self.model(input_tensor, commands_tensor)\n",
        "        chosen_action_index = tf.squeeze(indexes).numpy()\n",
        "\n",
        "\n",
        "        action = infos[\"admissible_commands\"][chosen_action_index]\n",
        "        #print(action)\n",
        "        # test\n",
        "        if self.mode == \"test\":\n",
        "            if done:\n",
        "                print('DONE')\n",
        "                #self.model.reset_hidden(1)\n",
        "            return action\n",
        "\n",
        "        # Training logic\n",
        "        self.no_train_step += 1\n",
        "\n",
        "        if self.transitions:\n",
        "            # Calculate reward\n",
        "            reward = score - self.last_score\n",
        "            self.last_score = score\n",
        "            if infos[\"won\"]:\n",
        "                reward += 100\n",
        "            if infos[\"lost\"]:\n",
        "                reward -= 100\n",
        "            print(reward)\n",
        "            # Update the last transition with the calculated reward\n",
        "            self.transitions[-1][0] = reward\n",
        "\n",
        "        # Store transition for training\n",
        "        self.transitions.append([None, indexes, outputs, value])\n",
        "        # Perform training at specified frequency\n",
        "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
        "            self._train_loop()\n",
        "\n",
        "        if done:\n",
        "            self.last_score = 0\n",
        "            self.model.reset_hidden(1)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "840Ao-bVlGe1"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nagent = NeuralAgent()\n",
        "nagent.train()\n",
        "play(nagent, 'games/tw-rewardsDense_goalDetailed.z8', max_step=50, nb_episodes=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "Ukf7uZA1qb-X",
        "outputId": "9fe80216-830c-49c2-d7bf-b320c117b153"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z80\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "TRAIN_MODEL METHOD\n",
            "[[0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[3]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087634, 0.00087634, 0.00087634, 0.00087634, 0.00087634,\n",
            "        0.00087634, 0.00087634, 0.00087634]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[6]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087634, 0.00087634, 0.00087634, 0.00087634, 0.00087634,\n",
            "        0.00087634, 0.00087634, 0.00087634]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[6]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087634, 0.00087634, 0.00087634, 0.00087634, 0.00087634,\n",
            "        0.00087634, 0.00087634, 0.00087634]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[6]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087633, 0.00087633, 0.00087633, 0.00087633, 0.00087633,\n",
            "        0.00087633, 0.00087633, 0.00087633]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[6]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087633, 0.00087633, 0.00087633, 0.00087633, 0.00087633,\n",
            "        0.00087633, 0.00087633, 0.00087633]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[2]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087633, 0.00087633, 0.00087633, 0.00087633, 0.00087633,\n",
            "        0.00087633, 0.00087633, 0.00087633]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[3]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087634, 0.00087634, 0.00087634, 0.00087634, 0.00087634,\n",
            "        0.00087634, 0.00087634, 0.00087634]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[6]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087634, 0.00087634, 0.00087634, 0.00087634, 0.00087634,\n",
            "        0.00087634, 0.00087634, 0.00087634]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [0, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[0]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087633, 0.00087633, 0.00087633, 0.00087633, 0.00087633,\n",
            "        0.00087633, 0.00087633, 0.00087633]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>], [None, <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[1]])>, <tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
            "array([[0.00087634, 0.00087634, 0.00087634, 0.00087634, 0.00087634,\n",
            "        0.00087634, 0.00087634, 0.00087634]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00642286]], dtype=float32)>]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-63643dc85426>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnagent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnagent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'games/tw-rewardsDense_goalDetailed.z8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-dd95e4b8966c>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(agent, path, max_step, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnb_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# CHANGE FOR TF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m#command = agent.act(obs, done, infos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-a506f4ffd3d5>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, score, done, infos)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# Perform training at specified frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_train_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUPDATE_FREQUENCY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-a506f4ffd3d5>\u001b[0m in \u001b[0;36m_train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0;31m# Convert lists to TensorFlow tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m       \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming actions are already tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't convert Python sequence with mixed types to Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVFqoeYQhsr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rangent = RandomAgent()\n",
        "play(rangent, 'games/tw-rewardsDense_goalDetailed.z8', max_step=50, nb_episodes=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEwbjcwKw2jd",
        "outputId": "20c16b74-ffa1-4778-e19e-149a85b3a924"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..........  \tavg. steps:  50.0; avg. score:  2.6 / 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5VcJNXqGw2r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch code\n",
        "import re\n",
        "from typing import List, Mapping, Any, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textworld\n",
        "import textworld.gym\n",
        "from textworld import EnvInfos\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class CommandScorer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CommandScorer, self).__init__()\n",
        "        torch.manual_seed(42)  # For reproducibility\n",
        "        self.embedding    = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
        "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
        "        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n",
        "        self.hidden_size  = hidden_size\n",
        "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
        "        self.critic       = nn.Linear(hidden_size, 1)\n",
        "        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, obs, commands, **kwargs):\n",
        "        input_length = obs.size(0)\n",
        "        batch_size = obs.size(1)\n",
        "        nb_cmds = commands.size(1)\n",
        "\n",
        "        embedded = self.embedding(obs)\n",
        "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
        "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
        "        self.state_hidden = state_hidden\n",
        "        value = self.critic(state_output)\n",
        "\n",
        "        # Attention network over the commands.\n",
        "        cmds_embedding = self.embedding.forward(commands)\n",
        "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
        "\n",
        "        # Same observed state for all commands.\n",
        "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
        "\n",
        "        # Same command choices for the whole batch.\n",
        "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
        "\n",
        "        # Concatenate the observed state and command encodings.\n",
        "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
        "\n",
        "        # Compute one score per command.\n",
        "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
        "\n",
        "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
        "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
        "        return scores, index, value\n",
        "\n",
        "    def reset_hidden(self, batch_size):\n",
        "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class NeuralAgent:\n",
        "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
        "    MAX_VOCAB_SIZE = 1000\n",
        "    UPDATE_FREQUENCY = 10\n",
        "    LOG_FREQUENCY = 1000\n",
        "    GAMMA = 0.9\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._initialized = False\n",
        "        self._epsiode_has_started = False\n",
        "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
        "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
        "\n",
        "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
        "\n",
        "        self.mode = \"test\"\n",
        "\n",
        "    def train(self):\n",
        "        self.mode = \"train\"\n",
        "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "        self.transitions = []\n",
        "        self.model.reset_hidden(1)\n",
        "        self.last_score = 0\n",
        "        self.no_train_step = 0\n",
        "\n",
        "    def test(self):\n",
        "        self.mode = \"test\"\n",
        "        self.model.reset_hidden(1)\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> EnvInfos:\n",
        "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
        "                        won=True, lost=True)\n",
        "\n",
        "    def _get_word_id(self, word):\n",
        "        if word not in self.word2id:\n",
        "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
        "                return self.word2id[\"<UNK>\"]\n",
        "\n",
        "            self.id2word.append(word)\n",
        "            self.word2id[word] = len(self.word2id)\n",
        "\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
        "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
        "        word_ids = list(map(self._get_word_id, text.split()))\n",
        "        return word_ids\n",
        "\n",
        "    def _process(self, texts):\n",
        "        texts = list(map(self._tokenize, texts))\n",
        "        max_len = max(len(l) for l in texts)\n",
        "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            padded[i, :len(text)] = text\n",
        "\n",
        "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
        "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
        "        return padded_tensor\n",
        "\n",
        "    def _discount_rewards(self, last_values):\n",
        "        returns, advantages = [], []\n",
        "        R = last_values.data\n",
        "        for t in reversed(range(len(self.transitions))):\n",
        "            rewards, _, _, values = self.transitions[t]\n",
        "            R = rewards + self.GAMMA * R\n",
        "            adv = R - values\n",
        "            returns.append(R)\n",
        "            advantages.append(adv)\n",
        "\n",
        "        return returns[::-1], advantages[::-1]\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
        "\n",
        "        # Build agent's observation: feedback + look + inventory.\n",
        "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
        "\n",
        "\n",
        "        # Tokenize and pad the input and the commands to chose from.\n",
        "        input_tensor = self._process([input_])\n",
        "        commands_tensor = self._process(infos[\"admissible_commands\"])\n",
        "\n",
        "        #print(commands_tensor)\n",
        "        # Get our next action and value prediction.\n",
        "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
        "        action = infos[\"admissible_commands\"][indexes[0]]\n",
        "\n",
        "        if self.mode == \"test\":\n",
        "            if done:\n",
        "                self.model.reset_hidden(1)\n",
        "            return action\n",
        "\n",
        "        self.no_train_step += 1\n",
        "\n",
        "        if self.transitions:\n",
        "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
        "            self.last_score = score\n",
        "            if infos[\"won\"]:\n",
        "                reward += 100\n",
        "            if infos[\"lost\"]:\n",
        "                reward -= 100\n",
        "\n",
        "            self.transitions[-1][0] = reward  # Update reward information.\n",
        "\n",
        "        self.stats[\"max\"][\"score\"].append(score)\n",
        "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
        "            # Update model\n",
        "            returns, advantages = self._discount_rewards(values)\n",
        "\n",
        "            loss = 0\n",
        "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
        "                reward, indexes_, outputs_, values_ = transition\n",
        "\n",
        "                advantage        = advantage.detach() # Block gradients flow here.\n",
        "                probs            = F.softmax(outputs_, dim=2)\n",
        "                log_probs        = torch.log(probs)\n",
        "                log_action_probs = log_probs.gather(2, indexes_)\n",
        "                policy_loss      = (-log_action_probs * advantage).sum()\n",
        "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
        "                entropy     = (-probs * log_probs).sum()\n",
        "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
        "\n",
        "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
        "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
        "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
        "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
        "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
        "\n",
        "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
        "                msg = \"{:6d}. \".format(self.no_train_step)\n",
        "                msg += \"  \".join(\"{}: {: 3.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
        "                msg += \"  \" + \"  \".join(\"{}: {:2d}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
        "                msg += \"  vocab: {:3d}\".format(len(self.id2word))\n",
        "                print(msg)\n",
        "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
        "\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            self.transitions = []\n",
        "            self.model.reset_hidden(1)\n",
        "        else:\n",
        "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
        "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
        "\n",
        "        if done:\n",
        "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
        "\n",
        "        return action"
      ],
      "metadata": {
        "id": "xkIdpXNIlGb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAiJSccOlGZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpwjqfgjlGPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVUQTX5lPDrb"
      },
      "source": [
        "# some code that works but i dont understand yet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-sjSdnhDeYM"
      },
      "outputs": [],
      "source": [
        "# create 7 games\n",
        "# Same as !make_games.sh\n",
        "!tw-make tw-simple --rewards dense    --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsDense_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards balanced --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalDetailed.z8\n",
        "!tw-make tw-simple --rewards dense    --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsDense_goalBrief.z8\n",
        "!tw-make tw-simple --rewards balanced --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalBrief.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalBrief.z8\n",
        "!tw-make tw-simple --rewards sparse   --goal none     --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalNone.z8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZBxhL534xM_"
      },
      "outputs": [],
      "source": [
        "!tw-make tw-simple --rewards dense    --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsDense_goalDetailed.z8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-gOKaM6Ec5a"
      },
      "outputs": [],
      "source": [
        "# building a random baseline\n",
        "class RandomAgent(textworld.gym.Agent):\n",
        "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
        "    def __init__(self, seed=1234):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.RandomState(self.seed)\n",
        "\n",
        "    @property\n",
        "    def infos_to_request(self) -> textworld.EnvInfos:\n",
        "        return textworld.EnvInfos(admissible_commands=True)\n",
        "\n",
        "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
        "        return self.rng.choice(infos[\"admissible_commands\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Gxi-VsElrL"
      },
      "outputs": [],
      "source": [
        "# play function\n",
        "def play(agent, path, max_step=30, nb_episodes=10, verbose=True):\n",
        "    # torch.manual_seed(20211021)  # For reproducibility when using action sampling.\n",
        "\n",
        "    infos_to_request = agent.infos_to_request\n",
        "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
        "\n",
        "    gamefiles = [path]\n",
        "    if os.path.isdir(path):\n",
        "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
        "\n",
        "    env_id = textworld.gym.register_games(gamefiles,\n",
        "                                          request_infos=infos_to_request,\n",
        "                                          max_episode_steps=max_step)\n",
        "    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            print(os.path.dirname(path), end=\"\")\n",
        "        else:\n",
        "            print(os.path.basename(path), end=\"\")\n",
        "\n",
        "    # Collect some statistics: nb_steps, final reward.\n",
        "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
        "    for no_episode in range(nb_episodes):\n",
        "        obs, infos = env.reset()  # Start new episode.\n",
        "\n",
        "        score = 0\n",
        "        done = False\n",
        "        nb_moves = 0\n",
        "        while not done:\n",
        "            command = agent.act(obs, score, done, infos)\n",
        "            obs, score, done, infos = env.step(command)\n",
        "            nb_moves += 1\n",
        "\n",
        "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
        "\n",
        "        if verbose:\n",
        "            print(\".\", end=\"\")\n",
        "        avg_moves.append(nb_moves)\n",
        "        avg_scores.append(score)\n",
        "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
        "\n",
        "    env.close()\n",
        "    if verbose:\n",
        "        if os.path.isdir(path):\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
        "        else:\n",
        "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
        "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFqOVbf3EmaX",
        "outputId": "2227bcb5-34c3-4b1c-d866-5d88c69046d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tw-rewardsDense_goalDetailed.z8..............................  \tavg. steps:  30.0; avg. score:  2.3 / 10.\n"
          ]
        }
      ],
      "source": [
        "# We report the score and steps averaged over 10 playthroughs.\n",
        "play(RandomAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")    # Dense rewards\n",
        "\n",
        "#play(RandomAgent(), \"./games/tw-rewardsBalanced_goalDetailed.z8\") # Balanced rewards\n",
        "#play(RandomAgent(), \"./games/tw-rewardsSparse_goalDetailed.z8\")   # Sparse rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# misc (some old code that i dont want to throw away\n"
      ],
      "metadata": {
        "id": "b2sRsoIZLZNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TENSORFLOW\n",
        "# TODO NEED TO MAKE THE OUTPUT SIZE OF THE MODEL DYNAMIC\n",
        "class CommandScorer(models.Model):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CommandScorer, self).__init__()\n",
        "        self.embedding = layers.Embedding(input_size, hidden_size)\n",
        "        self.encoder_gru = layers.GRU(hidden_size, return_sequences=True, return_state=True)\n",
        "        self.cmd_encoder_gru = layers.GRU(hidden_size, return_state=True)\n",
        "        self.state_gru = layers.GRU(hidden_size, return_state=True)\n",
        "        #self.critic = layers.Dense(1) # TODO IMPLEMENT IT\n",
        "        self.att_cmd = layers.Dense(1)\n",
        "\n",
        "    def call(self, obs, commands):\n",
        "        #print('COMMANDSCORER CALL METHOD')\n",
        "\n",
        "        print('commands', commands.shape)\n",
        "        embedded = self.embedding(obs)\n",
        "        #embedded = tf.repeat(embedded, repeats=commands.shape[0], axis=0)\n",
        "        print('embedded', embedded.shape)\n",
        "\n",
        "        #encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
        "        _, encoder_hidden = self.encoder_gru(embedded)\n",
        "        #encoder_hidden = tf.expand_dims(encoder_hidden, axis=1)\n",
        "\n",
        "        #print('encoder_output', encoder_output.shape)\n",
        "        print('encoder_hidden', encoder_hidden.shape)\n",
        "\n",
        "        cmds_embedding = self.embedding(commands) # (8, 3, 128)\n",
        "        print('cmds_embedding', cmds_embedding.shape)\n",
        "        # Initialize a list to store the encoding of each command\n",
        "        cmds_encoding_last_states_list = []\n",
        "\n",
        "        # Process each command independently\n",
        "        for i in range(cmds_embedding.shape[0]):  # Iterate over each command\n",
        "            # Treat each command as a single time-step sequence\n",
        "            cmd = tf.expand_dims(cmds_embedding[i], axis=0)  # Shape: (1, 3, 128)\n",
        "            _, cmd_encoding = self.cmd_encoder_gru(cmd)  # Shape: (1, 128)\n",
        "            cmd_encoding = tf.reshape(cmd_encoding, (1, 1, -1))\n",
        "            cmds_encoding_last_states_list.append(cmd_encoding)\n",
        "            \n",
        "\n",
        "        cmds_encoding_last_states = tf.concat(cmds_encoding_last_states_list, axis=1)  # Shape: (1, 8, 128)\n",
        "        print('cmds_encoding_last_states', cmds_encoding_last_states.shape)\n",
        "\n",
        "\n",
        "        # Reshape for batch processing: treat each command as a separate sequence\n",
        "        #cmds_embedding_reshaped = tf.reshape(cmds_embedding, (-1, cmds_embedding.shape[2]))  # Shape: (24, 128)\n",
        "        #cmds_embedding_reshaped = tf.expand_dims(cmds_embedding_reshaped, axis=0)  # Shape: (1, 24, 128)\n",
        "\n",
        "        #print('cmds_embedding_reshaped', cmds_embedding_reshaped.shape)\n",
        "        #_, cmds_encoding_last_states = self.cmd_encoder_gru(cmds_embedding_reshaped)  # Shape: (1, 128)\n",
        "        \n",
        "        # Reshape to get separate encodings for each command\n",
        "        #cmds_encoding_last_states = tf.reshape(cmds_encoding_last_states, (1, 8, 128))  # Shape: (1, 8, 128)\n",
        "\n",
        "        #print('cmds_encoding_last_states', cmds_encoding_last_states.shape)\n",
        "\n",
        "\n",
        "        #cmds_encoding_last_states, _ = self.cmd_encoder_gru(cmds_embedding)\n",
        "        #cmds_embedding_reshaped = tf.reshape(cmds_embedding, (1, -1, 128))  # Reshape to (1, 24, 128)\n",
        "\n",
        "        #print('cmds_embedding', cmds_embedding.shape)\n",
        "        #print('cmds_embedding_reshaped', cmds_embedding_reshaped.shape)\n",
        "\n",
        "        #_, cmds_encoding_last_states = self.cmd_encoder_gru(cmds_embedding_reshaped)  # Shape: (1, 128)\n",
        "        #print('cmds_encoding_last_states', cmds_encoding_last_states.shape)\n",
        "        #cmds_encoding_last_states = tf.reshape(cmds_encoding_last_states, (1, 1, 8, 128))\n",
        "\n",
        "        #print('cmds_encoding_last_states reshaped', cmds_encoding_last_states.shape)\n",
        "\n",
        "\n",
        "        #state_output, state_hidden = self.state_gru(encoder_hidden)\n",
        "\n",
        "        #print('state_output', state_output.shape)\n",
        "        #print('state_hidden', state_hidden.shape)\n",
        "\n",
        "        # Dynamically create scoring layer based on number of commands\n",
        "        #num_commands = tf.shape(commands)[1] # ATTEMPT FOR DYNAMIC ACTION SPACE\n",
        "        #att_cmd = layers.Dense(num_commands) # ATTEMPT FOR DYNAMIC ACTION SPACE\n",
        "\n",
        "        # Repeat and concatenate for attention\n",
        "        #state_hidden_repeated = tf.repeat(tf.expand_dims(state_hidden, axis=1), repeats=cmds_encoding_last_states.shape[1], axis=1)\n",
        "\n",
        "        state_hidden_repeated = tf.repeat(tf.expand_dims(encoder_hidden, axis=1), repeats=8, axis=1)  # Shape: (1, 8, 128)\n",
        "        #state_hidden_repeated = tf.expand_dims(state_hidden_repeated, axis=1) # Shape: (1, 1, 8, 256)\n",
        "\n",
        "\n",
        "        print('state_hidden_repeated', state_hidden_repeated.shape)\n",
        "        \n",
        "        #cmds_encoding_last_states = tf.expand_dims(cmds_encoding_last_states, axis=2)  # Shape becomes [9, 128, 1]\n",
        "        #cmd_selector_input = tf.concat([state_hidden_repeated, cmds_encoding_last_states], axis=-1)  # Now concatenation should work\n",
        "\n",
        "        cmd_selector_input = tf.concat([state_hidden_repeated, cmds_encoding_last_states], axis=-1)  # Shape: (1, 1, 8, 256)\n",
        "\n",
        "        #print('cmds_encoding_last_states', cmds_encoding_last_states.shape)\n",
        "        print('cmd_selector_input', cmd_selector_input.shape)\n",
        "        \n",
        "\n",
        "\n",
        "        scores = self.att_cmd(cmd_selector_input) # DYNAMIC ATTEMPT\n",
        "        scores = tf.squeeze(scores, axis=-1)\n",
        "\n",
        "        # Calculate probabilities and sample an action\n",
        "        probs = tf.nn.softmax(scores, axis=1)\n",
        "        index = tf.random.categorical(tf.math.log(probs), num_samples=1)  # Use log probabilities for stability\n",
        "\n",
        "        #probs = tf.nn.softmax(scores, axis=1)\n",
        "        #probs = tf.squeeze(probs, axis=-1)\n",
        "        #index = tf.random.categorical(probs, num_samples=1)\n",
        "\n",
        "        return scores, index\n"
      ],
      "metadata": {
        "id": "Nb8j9Cy1Lrli"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeUkaKm9Llft"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T1onOUgXTEFM",
        "2VbO-8D4uJul"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2V8duiyrbvB3qaX/WpvtP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}